{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Comparaison des Mod√®les Fine-tun√©s pour Question-Answering\n",
        "\n",
        "**Cours:** M2 Datascale - Fouille de Donn√©es  \n",
        "**Objectif:** Comparer les performances de plusieurs mod√®les fine-tun√©s sur SQuAD v1.1\n",
        "\n",
        "Ce notebook charge les r√©sultats sauvegard√©s par les diff√©rents mod√®les et g√©n√®re :\n",
        "- Tableaux comparatifs\n",
        "- Visualisations\n",
        "- Analyse des trade-offs\n",
        "- Recommandations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "## 1. Installation et Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install",
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q matplotlib seaborn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Style pour les graphiques\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "**Modifiez cette section selon les mod√®les que vous avez entra√Æn√©s**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# R√©pertoire contenant les mod√®les fine-tun√©s\n",
        "MODELS_DIR = \"./models\"\n",
        "\n",
        "# Liste des mod√®les √† comparer (noms des dossiers)\n",
        "# Ajoutez/retirez selon vos besoins\n",
        "MODEL_FOLDERS = [\n",
        "    \"distilbert-base-uncased_squad\",\n",
        "    \"bert-base-uncased_squad\",\n",
        "    \"roberta-base_squad\",\n",
        "\n",
        "]\n",
        "\n",
        "# Noms courts pour l'affichage\n",
        "MODEL_DISPLAY_NAMES = {\n",
        "    \"distilbert-base-uncased_squad\": \"DistilBERT\",\n",
        "    \"bert-base-uncased_squad\": \"BERT\",\n",
        "    \"roberta-base_squad\": \"RoBERTa\",\n",
        "    \"albert-base-v2_squad\": \"ALBERT\",\n",
        "    \"deberta-v3-base_squad\": \"DeBERTa\",\n",
        "}\n",
        "\n",
        "print(f\"Recherche des mod√®les dans: {MODELS_DIR}\")\n",
        "print(f\"Mod√®les √† comparer: {len(MODEL_FOLDERS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "## 3. Chargement des R√©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-results",
      "metadata": {
        "id": "load-results"
      },
      "outputs": [],
      "source": [
        "def load_model_results(model_folder):\n",
        "    \"\"\"\n",
        "    Charge les r√©sultats d'un mod√®le depuis results.json\n",
        "    \"\"\"\n",
        "    results_path = os.path.join(MODELS_DIR, model_folder, \"results.json\")\n",
        "    \n",
        "    if not os.path.exists(results_path):\n",
        "        print(f\"‚ö†Ô∏è  Fichier non trouv√©: {results_path}\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        with open(results_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"‚úì Charg√©: {model_folder}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du chargement de {model_folder}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Charger tous les r√©sultats\n",
        "results_list = []\n",
        "\n",
        "for model_folder in MODEL_FOLDERS:\n",
        "    data = load_model_results(model_folder)\n",
        "    if data:\n",
        "        # Ajouter le nom d'affichage\n",
        "        data['display_name'] = MODEL_DISPLAY_NAMES.get(model_folder, model_folder)\n",
        "        results_list.append(data)\n",
        "\n",
        "print(f\"\\nTotal: {len(results_list)} mod√®les charg√©s avec succ√®s\")\n",
        "\n",
        "if len(results_list) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è Aucun r√©sultat trouv√©. V√©rifiez:\")\n",
        "    print(f\"   1. Le chemin MODELS_DIR: {MODELS_DIR}\")\n",
        "    print(f\"   2. Les noms dans MODEL_FOLDERS\")\n",
        "    print(f\"   3. Que les fichiers results.json existent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "explore-results",
      "metadata": {
        "id": "explore-results"
      },
      "outputs": [],
      "source": [
        "# Voir un exemple de r√©sultats\n",
        "if results_list:\n",
        "    print(\"Exemple de donn√©es charg√©es:\")\n",
        "    print(json.dumps(results_list[0], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "## 4. Cr√©ation du DataFrame de Comparaison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-dataframe",
      "metadata": {
        "id": "create-dataframe"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    # Cr√©er le DataFrame\n",
        "    comparison_data = []\n",
        "    \n",
        "    for result in results_list:\n",
        "        comparison_data.append({\n",
        "            'Mod√®le': result['display_name'],\n",
        "            'Param√®tres (M)': result['total_parameters'] / 1e6,\n",
        "            'F1 (%)': result['f1'],\n",
        "            'Exact Match (%)': result['exact_match'],\n",
        "            'Temps Entra√Ænement (min)': result['training_time_minutes'],\n",
        "            'Inf√©rence (ms)': result['avg_inference_time_ms'],\n",
        "            '√âchantillons Train': result['num_train_samples'],\n",
        "            '√âchantillons Eval': result['num_eval_samples'],\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    # Trier par F1 d√©croissant\n",
        "    df = df.sort_values('F1 (%)', ascending=False).reset_index(drop=True)\n",
        "    \n",
        "    print(\"DataFrame cr√©√© avec succ√®s\")\n",
        "    print(f\"Dimensions: {df.shape}\")\n",
        "else:\n",
        "    print(\"Aucune donn√©e √† afficher\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "## 5. Tableau Comparatif Principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "main-table",
      "metadata": {
        "id": "main-table"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    print(\"=\"*80)\n",
        "    print(\"TABLEAU COMPARATIF DES MOD√àLES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Afficher le tableau principal\n",
        "    display_df = df[['Mod√®le', 'Param√®tres (M)', 'F1 (%)', 'Exact Match (%)', \n",
        "                     'Temps Entra√Ænement (min)', 'Inf√©rence (ms)']].copy()\n",
        "    \n",
        "    # Formater les nombres\n",
        "    display_df['Param√®tres (M)'] = display_df['Param√®tres (M)'].apply(lambda x: f\"{x:.1f}\")\n",
        "    display_df['F1 (%)'] = display_df['F1 (%)'].apply(lambda x: f\"{x:.2f}\")\n",
        "    display_df['Exact Match (%)'] = display_df['Exact Match (%)'].apply(lambda x: f\"{x:.2f}\")\n",
        "    display_df['Temps Entra√Ænement (min)'] = display_df['Temps Entra√Ænement (min)'].apply(lambda x: f\"{x:.1f}\")\n",
        "    display_df['Inf√©rence (ms)'] = display_df['Inf√©rence (ms)'].apply(lambda x: f\"{x:.1f}\")\n",
        "    \n",
        "    print(display_df.to_string(index=False))\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "markdown-table",
      "metadata": {
        "id": "markdown-table"
      },
      "outputs": [],
      "source": [
        "# G√©n√©rer le tableau au format Markdown (pour le rapport)\n",
        "if results_list:\n",
        "    print(\"\\nTableau au format Markdown (copier pour le rapport):\")\n",
        "    print(\"=\"*80)\n",
        "    print(display_df.to_markdown(index=False))\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "## 6. Statistiques Descriptives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stats",
      "metadata": {
        "id": "stats"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    print(\"Statistiques descriptives:\\n\")\n",
        "    \n",
        "    stats_df = df[['F1 (%)', 'Exact Match (%)', 'Temps Entra√Ænement (min)', 'Inf√©rence (ms)']]\n",
        "    print(stats_df.describe().round(2))\n",
        "    \n",
        "    print(\"\\nR√©sum√©:\")\n",
        "    print(f\"  Meilleur F1: {df.loc[0, 'Mod√®le']} ({df.loc[0, 'F1 (%)']:.2f}%)\")\n",
        "    print(f\"  Meilleur EM: {df.loc[df['Exact Match (%)'].idxmax(), 'Mod√®le']} ({df['Exact Match (%)'].max():.2f}%)\")\n",
        "    print(f\"  Plus rapide (entra√Ænement): {df.loc[df['Temps Entra√Ænement (min)'].idxmin(), 'Mod√®le']} ({df['Temps Entra√Ænement (min)'].min():.1f} min)\")\n",
        "    print(f\"  Plus rapide (inf√©rence): {df.loc[df['Inf√©rence (ms)'].idxmin(), 'Mod√®le']} ({df['Inf√©rence (ms)'].min():.1f} ms)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "## 7. Visualisations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subsection-7-1",
      "metadata": {
        "id": "subsection-7-1"
      },
      "source": [
        "### 7.1 Comparaison F1 et Exact Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-f1-em",
      "metadata": {
        "id": "plot-f1-em"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    x = np.arange(len(df))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars1 = ax.bar(x - width/2, df['F1 (%)'], width, label='F1 Score', color='#3498db')\n",
        "    bars2 = ax.bar(x + width/2, df['Exact Match (%)'], width, label='Exact Match', color='#e74c3c')\n",
        "    \n",
        "    ax.set_xlabel('Mod√®le', fontweight='bold')\n",
        "    ax.set_ylabel('Score (%)', fontweight='bold')\n",
        "    ax.set_title('Comparaison F1 Score et Exact Match', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(df['Mod√®le'], rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.1f}',\n",
        "                   ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comparison_f1_em.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Graphique sauvegard√©: comparison_f1_em.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subsection-7-2",
      "metadata": {
        "id": "subsection-7-2"
      },
      "source": [
        "### 7.2 Trade-off Performance vs Vitesse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-tradeoff",
      "metadata": {
        "id": "plot-tradeoff"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    # Scatter plot avec taille = nombre de param√®tres\n",
        "    scatter = ax.scatter(df['Inf√©rence (ms)'], df['F1 (%)'], \n",
        "                        s=df['Param√®tres (M)']*5,  # Taille proportionnelle aux params\n",
        "                        alpha=0.6, c=range(len(df)), cmap='viridis')\n",
        "    \n",
        "    # Ajouter les labels\n",
        "    for idx, row in df.iterrows():\n",
        "        ax.annotate(row['Mod√®le'], \n",
        "                   (row['Inf√©rence (ms)'], row['F1 (%)']),\n",
        "                   xytext=(5, 5), textcoords='offset points',\n",
        "                   fontsize=10, fontweight='bold')\n",
        "    \n",
        "    ax.set_xlabel('Temps d\\'inf√©rence (ms)', fontweight='bold')\n",
        "    ax.set_ylabel('F1 Score (%)', fontweight='bold')\n",
        "    ax.set_title('Trade-off Performance vs Vitesse\\n(Taille des bulles = nombre de param√®tres)', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Zones id√©ales\n",
        "    ax.axhline(y=df['F1 (%)'].mean(), color='red', linestyle='--', alpha=0.3, label='F1 moyen')\n",
        "    ax.axvline(x=df['Inf√©rence (ms)'].mean(), color='blue', linestyle='--', alpha=0.3, label='Temps moyen')\n",
        "    ax.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('tradeoff_performance_speed.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Graphique sauvegard√©: tradeoff_performance_speed.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subsection-7-3",
      "metadata": {
        "id": "subsection-7-3"
      },
      "source": [
        "### 7.3 Temps d'Entra√Ænement vs Param√®tres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-training-time",
      "metadata": {
        "id": "plot-training-time"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
        "    bars = ax.bar(df['Mod√®le'], df['Temps Entra√Ænement (min)'], \n",
        "                  color=colors[:len(df)], alpha=0.7)\n",
        "    \n",
        "    ax.set_ylabel('Temps d\\'entra√Ænement (minutes)', fontweight='bold')\n",
        "    ax.set_title('Temps d\\'Entra√Ænement par Mod√®le', fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Ajouter les valeurs et nombre de param√®tres\n",
        "    for idx, (bar, row) in enumerate(zip(bars, df.itertuples())):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{height:.1f} min\\n({row._2:.0f}M params)',\n",
        "               ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_time_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Graphique sauvegard√©: training_time_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subsection-7-4",
      "metadata": {
        "id": "subsection-7-4"
      },
      "source": [
        "### 7.4 Heatmap des M√©triques Normalis√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-heatmap",
      "metadata": {
        "id": "plot-heatmap"
      },
      "outputs": [],
      "source": [
        "if results_list and len(df) > 1:\n",
        "    # Pr√©parer les donn√©es pour la heatmap\n",
        "    heatmap_data = df[['Mod√®le', 'F1 (%)', 'Exact Match (%)', 'Inf√©rence (ms)']].copy()\n",
        "    heatmap_data = heatmap_data.set_index('Mod√®le')\n",
        "    \n",
        "    # Inverser le temps d'inf√©rence (plus bas = mieux)\n",
        "    heatmap_data['Vitesse (inverse ms)'] = 1 / heatmap_data['Inf√©rence (ms)'] * 100\n",
        "    heatmap_data = heatmap_data.drop('Inf√©rence (ms)', axis=1)\n",
        "    \n",
        "    # Normaliser entre 0 et 100\n",
        "    normalized = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min()) * 100\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    \n",
        "    sns.heatmap(normalized.T, annot=True, fmt='.1f', cmap='RdYlGn', \n",
        "                cbar_kws={'label': 'Score Normalis√© (0-100)'},\n",
        "                linewidths=0.5, ax=ax)\n",
        "    \n",
        "    ax.set_title('Comparaison Multi-Crit√®res (Normalis√©)', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('M√©trique', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('heatmap_normalized.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Graphique sauvegard√©: heatmap_normalized.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-8",
      "metadata": {
        "id": "section-8"
      },
      "source": [
        "## 8. Analyse des R√©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "analysis",
      "metadata": {
        "id": "analysis"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    print(\"=\"*80)\n",
        "    print(\"ANALYSE DES R√âSULTATS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # 1. Meilleur mod√®le global (F1)\n",
        "    best_f1 = df.loc[0]\n",
        "    print(f\"\\n1. MEILLEUR F1 SCORE: {best_f1['Mod√®le']}\")\n",
        "    print(f\"   - F1: {best_f1['F1 (%)']:.2f}%\")\n",
        "    print(f\"   - Exact Match: {best_f1['Exact Match (%)']:.2f}%\")\n",
        "    print(f\"   - Param√®tres: {best_f1['Param√®tres (M)']:.1f}M\")\n",
        "    \n",
        "    # 2. Mod√®le le plus rapide\n",
        "    fastest = df.loc[df['Inf√©rence (ms)'].idxmin()]\n",
        "    print(f\"\\n2. PLUS RAPIDE (INF√âRENCE): {fastest['Mod√®le']}\")\n",
        "    print(f\"   - Temps: {fastest['Inf√©rence (ms)']:.1f} ms\")\n",
        "    print(f\"   - F1: {fastest['F1 (%)']:.2f}%\")\n",
        "    print(f\"   - Trade-off: {fastest['F1 (%)'] / fastest['Inf√©rence (ms)']:.2f} F1/ms\")\n",
        "    \n",
        "    # 3. Meilleur rapport qualit√©/vitesse\n",
        "    df['Efficacit√©'] = df['F1 (%)'] / df['Inf√©rence (ms)']\n",
        "    most_efficient = df.loc[df['Efficacit√©'].idxmax()]\n",
        "    print(f\"\\n3. MEILLEUR RAPPORT QUALIT√â/VITESSE: {most_efficient['Mod√®le']}\")\n",
        "    print(f\"   - Efficacit√©: {most_efficient['Efficacit√©']:.2f} F1/ms\")\n",
        "    print(f\"   - F1: {most_efficient['F1 (%)']:.2f}%\")\n",
        "    print(f\"   - Inf√©rence: {most_efficient['Inf√©rence (ms)']:.1f} ms\")\n",
        "    \n",
        "    # 4. Diff√©rences de performance\n",
        "    if len(df) > 1:\n",
        "        f1_gap = df['F1 (%)'].max() - df['F1 (%)'].min()\n",
        "        em_gap = df['Exact Match (%)'].max() - df['Exact Match (%)'].min()\n",
        "        speed_gap = df['Inf√©rence (ms)'].max() - df['Inf√©rence (ms)'].min()\n",
        "        \n",
        "        print(f\"\\n4. √âCARTS OBSERV√âS:\")\n",
        "        print(f\"   - F1: {f1_gap:.2f} points ({f1_gap/df['F1 (%)'].mean()*100:.1f}%)\")\n",
        "        print(f\"   - Exact Match: {em_gap:.2f} points ({em_gap/df['Exact Match (%)'].mean()*100:.1f}%)\")\n",
        "        print(f\"   - Vitesse: {speed_gap:.1f} ms ({speed_gap/df['Inf√©rence (ms)'].mean()*100:.1f}%)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-9",
      "metadata": {
        "id": "section-9"
      },
      "source": [
        "## 9. Recommandations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recommendations",
      "metadata": {
        "id": "recommendations"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    print(\"=\"*80)\n",
        "    print(\"RECOMMANDATIONS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(\"\\nSelon le cas d'usage:\\n\")\n",
        "    \n",
        "    # Cas 1: Performance maximale\n",
        "    best_perf = df.loc[0]\n",
        "    print(f\"üìä PERFORMANCE MAXIMALE:\")\n",
        "    print(f\"   ‚Üí {best_perf['Mod√®le']}\")\n",
        "    print(f\"   Raison: Meilleur F1 ({best_perf['F1 (%)']:.2f}%)\")\n",
        "    print(f\"   Compromis: {best_perf['Inf√©rence (ms)']:.1f} ms d'inf√©rence\\n\")\n",
        "    \n",
        "    # Cas 2: Latence minimale\n",
        "    fastest = df.loc[df['Inf√©rence (ms)'].idxmin()]\n",
        "    print(f\"‚ö° TEMPS R√âEL / LATENCE MINIMALE:\")\n",
        "    print(f\"   ‚Üí {fastest['Mod√®le']}\")\n",
        "    print(f\"   Raison: Plus rapide ({fastest['Inf√©rence (ms)']:.1f} ms)\")\n",
        "    print(f\"   Compromis: F1 de {fastest['F1 (%)']:.2f}% (vs {best_perf['F1 (%)']:.2f}% max)\\n\")\n",
        "    \n",
        "    # Cas 3: √âquilibre\n",
        "    balanced = df.loc[df['Efficacit√©'].idxmax()]\n",
        "    print(f\"‚öñÔ∏è  √âQUILIBRE PERFORMANCE/VITESSE:\")\n",
        "    print(f\"   ‚Üí {balanced['Mod√®le']}\")\n",
        "    print(f\"   Raison: Meilleur ratio qualit√©/vitesse ({balanced['Efficacit√©']:.2f} F1/ms)\")\n",
        "    print(f\"   Avantages: F1={balanced['F1 (%)']:.2f}%, Temps={balanced['Inf√©rence (ms)']:.1f}ms\\n\")\n",
        "    \n",
        "    # Cas 4: Ressources limit√©es\n",
        "    lightest = df.loc[df['Param√®tres (M)'].idxmin()]\n",
        "    print(f\"üíæ RESSOURCES LIMIT√âES (Mobile/Edge):\")\n",
        "    print(f\"   ‚Üí {lightest['Mod√®le']}\")\n",
        "    print(f\"   Raison: Moins de param√®tres ({lightest['Param√®tres (M)']:.1f}M)\")\n",
        "    print(f\"   Performance: F1={lightest['F1 (%)']:.2f}%\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-10",
      "metadata": {
        "id": "section-10"
      },
      "source": [
        "## 10. Export des R√©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export",
      "metadata": {
        "id": "export"
      },
      "outputs": [],
      "source": [
        "if results_list:\n",
        "    # Sauvegarder le DataFrame en CSV\n",
        "    csv_path = \"comparison_results.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"‚úì Tableau sauvegard√©: {csv_path}\")\n",
        "    \n",
        "    # Sauvegarder en Excel\n",
        "    try:\n",
        "        excel_path = \"comparison_results.xlsx\"\n",
        "        df.to_excel(excel_path, index=False, sheet_name=\"Comparaison\")\n",
        "        print(f\"‚úì Tableau sauvegard√©: {excel_path}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è  Excel non disponible (pip install openpyxl pour activer)\")\n",
        "    \n",
        "    # Sauvegarder un rapport JSON complet\n",
        "    report = {\n",
        "        \"date_comparison\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"num_models\": len(results_list),\n",
        "        \"models_compared\": [r['model_name'] for r in results_list],\n",
        "        \"best_f1\": {\n",
        "            \"model\": best_f1['Mod√®le'],\n",
        "            \"score\": float(best_f1['F1 (%)'])\n",
        "        },\n",
        "        \"fastest\": {\n",
        "            \"model\": fastest['Mod√®le'],\n",
        "            \"inference_time_ms\": float(fastest['Inf√©rence (ms)'])\n",
        "        },\n",
        "        \"most_efficient\": {\n",
        "            \"model\": balanced['Mod√®le'],\n",
        "            \"efficiency\": float(balanced['Efficacit√©'])\n",
        "        },\n",
        "        \"detailed_results\": results_list\n",
        "    }\n",
        "    \n",
        "    json_path = \"comparison_report.json\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "    print(f\"‚úì Rapport complet: {json_path}\")\n",
        "    \n",
        "    print(\"\\nFichiers g√©n√©r√©s:\")\n",
        "    print(\"  - comparison_results.csv\")\n",
        "    print(\"  - comparison_results.xlsx (si disponible)\")\n",
        "    print(\"  - comparison_report.json\")\n",
        "    print(\"  - comparison_f1_em.png\")\n",
        "    print(\"  - tradeoff_performance_speed.png\")\n",
        "    print(\"  - training_time_comparison.png\")\n",
        "    print(\"  - heatmap_normalized.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-11",
      "metadata": {
        "id": "section-11"
      },
      "source": [
        "## 11. T√©l√©chargement (Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download",
      "metadata": {
        "id": "download"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    # T√©l√©charger tous les fichiers g√©n√©r√©s\n",
        "    files_to_download = [\n",
        "        \"comparison_results.csv\",\n",
        "        \"comparison_report.json\",\n",
        "        \"comparison_f1_em.png\",\n",
        "        \"tradeoff_performance_speed.png\",\n",
        "        \"training_time_comparison.png\",\n",
        "        \"heatmap_normalized.png\",\n",
        "    ]\n",
        "    \n",
        "    for file in files_to_download:\n",
        "        if os.path.exists(file):\n",
        "            files.download(file)\n",
        "            print(f\"‚úì T√©l√©charg√©: {file}\")\n",
        "            \n",
        "except ImportError:\n",
        "    print(\"Environnement local - fichiers disponibles dans le r√©pertoire courant\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
