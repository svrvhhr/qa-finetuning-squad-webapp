{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02905a5c",
   "metadata": {},
   "source": [
    "# Notebook 3 : Fine-tuning de RoBERTa pour Question-Answering\n",
    "\n",
    "**Cours:** M2 Datascale - Fouille de Donn√©es  \n",
    "\n",
    "## Objectifs\n",
    "- Fine-tuner le mod√®le RoBERTa-base sur SQuAD v1.1\n",
    "- √âvaluer avec les m√©triques F1 Score et Exact Match\n",
    "- Mesurer le temps d'inf√©rence\n",
    "- Comparer avec DistilBERT\n",
    "\n",
    "## Mod√®le\n",
    "- **Architecture:** RoBERTa (Liu et al., 2019)\n",
    "- **Param√®tres:** 125M\n",
    "- **Caract√©ristiques:** Optimisation de BERT avec plus de donn√©es et meilleur pr√©-entra√Ænement\n",
    "\n",
    "## R√©f√©rences\n",
    "- Liu et al. (2019). \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"\n",
    "- Rajpurkar et al. (2016). \"SQuAD: 100,000+ Questions for Machine Comprehension\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14151aa",
   "metadata": {},
   "source": [
    "## 1. V√©rification de l'Environnement GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedf64fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√©rification de l'environnement GPU...\n",
      "GPU d√©tect√©: NVIDIA GeForce RTX 5080\n",
      "M√©moire disponible: 16.60 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"V√©rification de l'environnement GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    device_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU d√©tect√©: {device_name}\")\n",
    "    print(f\"M√©moire disponible: {device_memory:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Aucun GPU d√©tect√©. Utilisation du CPU.\")\n",
    "    print(\"Note: L'entra√Ænement sur CPU sera significativement plus lent.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acae6a",
   "metadata": {},
   "source": [
    "## 2. Installation des D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4f8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate accelerate torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c1679",
   "metadata": {},
   "source": [
    "## 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c466f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khaledbouabdallah/Projects/qa-finetuning-squad-webapp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DefaultDataCollator,\n",
    "    pipeline\n",
    ")\n",
    "import evaluate\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e799d9",
   "metadata": {},
   "source": [
    "## 4. Configuration des Hyperparam√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b3327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Configuration des hyperparam√®tres - RoBERTa\n",
      "================================================================================\n",
      "Mod√®le: roberta-base\n",
      "Longueur maximale: 384 tokens\n",
      "Stride: 128 tokens\n",
      "Learning rate: 3e-05\n",
      "Nombre d'epochs: 3\n",
      "Batch size: 32\n",
      "√âchantillons entra√Ænement: 87599\n",
      "√âchantillons validation: 10570\n"
     ]
    }
   ],
   "source": [
    "# Seed pour la reproductibilit√©\n",
    "SEED = 42\n",
    "\n",
    "# Configuration du mod√®le\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "OUTPUT_DIR = \"../models/roberta_squad_finetuned\"\n",
    "\n",
    "# Hyperparam√®tres de tokenisation\n",
    "MAX_LENGTH = 384\n",
    "DOC_STRIDE = 128\n",
    "\n",
    "# Hyperparam√®tres d'entra√Ænement\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_EPOCHS = 3\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Taille du dataset\n",
    "USE_FULL_DATASET = True  # Mettre False pour test rapide\n",
    "MAX_TRAIN_SAMPLES = 87599 if USE_FULL_DATASET else 5000\n",
    "MAX_EVAL_SAMPLES = 10570 if USE_FULL_DATASET else 1000\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Configuration des hyperparam√®tres - RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mod√®le: {MODEL_NAME}\")\n",
    "print(f\"Longueur maximale: {MAX_LENGTH} tokens\")\n",
    "print(f\"Stride: {DOC_STRIDE} tokens\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Nombre d'epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"√âchantillons entra√Ænement: {MAX_TRAIN_SAMPLES}\")\n",
    "print(f\"√âchantillons validation: {MAX_EVAL_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc5b98",
   "metadata": {},
   "source": [
    "## 5. Fixation du Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd40514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed fix√© √† 42 pour la reproductibilit√©.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Fixe le seed pour assurer la reproductibilit√©.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Valeur du seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Seed fix√© √† {SEED} pour la reproductibilit√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da058f53",
   "metadata": {},
   "source": [
    "## 6. Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f023b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Chargement du dataset SQuAD v1.1\n",
      "================================================================================\n",
      "√âchantillons d'entra√Ænement: 87599\n",
      "√âchantillons de validation: 10570\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Chargement du dataset SQuAD v1.1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# S√©lection des sous-ensembles\n",
    "train_dataset = squad[\"train\"].shuffle(seed=SEED).select(\n",
    "    range(min(MAX_TRAIN_SAMPLES, len(squad[\"train\"])))\n",
    ")\n",
    "eval_dataset = squad[\"validation\"].shuffle(seed=SEED).select(\n",
    "    range(min(MAX_EVAL_SAMPLES, len(squad[\"validation\"])))\n",
    ")\n",
    "\n",
    "print(f\"√âchantillons d'entra√Ænement: {len(train_dataset)}\")\n",
    "print(f\"√âchantillons de validation: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69d3f8",
   "metadata": {},
   "source": [
    "## 7. Chargement du Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b257e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Chargement du tokenizer RoBERTa\n",
      "================================================================================\n",
      "Tokenizer charg√©: roberta-base\n",
      "Taille du vocabulaire: 50265\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Chargement du tokenizer RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "print(f\"Tokenizer charg√©: {MODEL_NAME}\")\n",
    "print(f\"Taille du vocabulaire: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e4df5",
   "metadata": {},
   "source": [
    "## 8. Pr√©paration des Donn√©es (Tokenisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0250ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    \"\"\"\n",
    "    Tokenise les exemples d'entra√Ænement et aligne les positions des r√©ponses.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): Batch d'exemples contenant questions, contextes et r√©ponses\n",
    "\n",
    "    Returns:\n",
    "        dict: Features tokenis√©es avec positions start/end\n",
    "    \"\"\"\n",
    "    # Supprimer les espaces √† gauche des questions (important pour RoBERTa)\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "\n",
    "        start_char = answers[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        if not (offsets[token_start_index][0] <= start_char and\n",
    "                offsets[token_end_index][1] >= end_char):\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    \"\"\"\n",
    "    Tokenise les exemples de validation en conservant les m√©tadonn√©es.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): Batch d'exemples de validation\n",
    "\n",
    "    Returns:\n",
    "        dict: Features tokenis√©es avec IDs d'exemples et offset mapping\n",
    "    \"\"\"\n",
    "    # Supprimer les espaces √† gauche des questions (important pour RoBERTa)\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized[\"input_ids\"])):\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        tokenized[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57607127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Tokenisation des donn√©es\n",
      "================================================================================\n",
      "Tokenisation de l'ensemble d'entra√Ænement...\n",
      "Tokenisation de l'ensemble de validation...\n",
      "\n",
      "Features d'entra√Ænement: 88568\n",
      "Features de validation: 10790\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Tokenisation des donn√©es\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Tokenisation de l'ensemble d'entra√Ænement...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(\"Tokenisation de l'ensemble de validation...\")\n",
    "tokenized_validation = eval_dataset.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")\n",
    "\n",
    "# Validation set pour eval_loss (doit contenir start_positions / end_positions)\n",
    "tokenized_validation_for_loss = eval_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures d'entra√Ænement: {len(tokenized_train)}\")\n",
    "print(f\"Features de validation: {len(tokenized_validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0cf236",
   "metadata": {},
   "source": [
    "## 9. Chargement du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ccc8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Initialisation du mod√®le RoBERTa\n",
      "================================================================================\n",
      "M√©moire GPU lib√©r√©e\n",
      "Mod√®le: roberta-base\n",
      "Param√®tres totaux: 124,056,578\n",
      "Param√®tres entra√Ænables: 124,056,578\n",
      "Device: cuda\n",
      "M√©moire GPU allou√©e: 0.50 GB\n",
      "M√©moire GPU r√©serv√©e: 0.53 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Initialisation du mod√®le RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Lib√©rer la m√©moire GPU si disponible\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"M√©moire GPU lib√©r√©e\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Mod√®le: {MODEL_NAME}\")\n",
    "print(f\"Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Afficher la m√©moire GPU utilis√©e\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"M√©moire GPU allou√©e: {allocated:.2f} GB\")\n",
    "    print(f\"M√©moire GPU r√©serv√©e: {reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f2cf9",
   "metadata": {},
   "source": [
    "## 10. Configuration de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f1fe5",
   "metadata": {},
   "source": [
    "## 9b. Optimisation M√©moire (optionnel)\n",
    "\n",
    "Si vous rencontrez des erreurs de m√©moire GPU, ex√©cutez cette cellule avant de continuer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121c4e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©moire GPU libre: 16.10 GB\n"
     ]
    }
   ],
   "source": [
    "# Activer le gradient checkpointing pour r√©duire l'utilisation m√©moire\n",
    "if hasattr(model.config, 'gradient_checkpointing'):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"‚úì Gradient checkpointing activ√©\")\n",
    "\n",
    "# R√©duire les batch sizes si n√©cessaire\n",
    "if torch.cuda.is_available():\n",
    "    available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1e9\n",
    "    free_memory = available_memory - allocated_memory\n",
    "\n",
    "    print(f\"M√©moire GPU libre: {free_memory:.2f} GB\")\n",
    "\n",
    "    # Si moins de 6GB disponibles, r√©duire les batch sizes\n",
    "    if free_memory < 6:\n",
    "        TRAIN_BATCH_SIZE = 8\n",
    "        EVAL_BATCH_SIZE = 16\n",
    "        print(f\"‚ö†Ô∏è  Batch sizes reduced: train={TRAIN_BATCH_SIZE}, eval={EVAL_BATCH_SIZE}\")\n",
    "\n",
    "    # Si moins de 4GB, encore plus petit\n",
    "    if free_memory < 4:\n",
    "        TRAIN_BATCH_SIZE = 4\n",
    "        EVAL_BATCH_SIZE = 8\n",
    "        print(f\"‚ö†Ô∏è  Batch sizes further reduced: train={TRAIN_BATCH_SIZE}, eval={EVAL_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40de921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer RoBERTa configur√© avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/results_roberta_squad\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation_for_loss,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DefaultDataCollator(),\n",
    ")\n",
    "\n",
    "print(\"Trainer RoBERTa configur√© avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee6827",
   "metadata": {},
   "source": [
    "## 11. Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad62d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "D√©but de l'entra√Ænement - RoBERTa\n",
      "================================================================================\n",
      "‚è±Ô∏è  Temps estim√©: ~70 minutes sur RTX 5080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8304/8304 1:41:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>0.855378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>0.831307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.880576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Entra√Ænement termin√©\n",
      "================================================================================\n",
      "Dur√©e totale: 101.91 minutes\n",
      "Loss finale: 0.8666\n",
      "\n",
      "Mod√®le sauvegard√© dans: ../models/roberta_squad_finetuned\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"D√©but de l'entra√Ænement - RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚è±Ô∏è  Temps estim√©: ~70 minutes sur RTX 5080\")\n",
    "\n",
    "start_time = time.time()\n",
    "train_result = trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Entra√Ænement termin√©\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dur√©e totale: {training_time/60:.2f} minutes\")\n",
    "print(f\"Loss finale: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Sauvegarde\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"\\nMod√®le sauvegard√© dans: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86635e9",
   "metadata": {},
   "source": [
    "## 12. √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "730e8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(examples, features, raw_predictions,\n",
    "                                n_best=20, max_answer_length=30):\n",
    "    \"\"\"\n",
    "    Post-traite les pr√©dictions brutes pour extraire les r√©ponses textuelles.\n",
    "\n",
    "    Args:\n",
    "        examples: Exemples originaux\n",
    "        features: Features tokenis√©es\n",
    "        raw_predictions: Logits de d√©but et fin\n",
    "        n_best: Nombre de candidats √† consid√©rer\n",
    "        max_answer_length: Longueur maximale de la r√©ponse\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping ID -> texte de r√©ponse\n",
    "    \"\"\"\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        context = example[\"context\"]\n",
    "\n",
    "        best_answer = {\"text\": \"\", \"score\": -float(\"inf\")}\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-n_best:][::-1]\n",
    "            end_indexes = np.argsort(end_logits)[-n_best:][::-1]\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (start_index >= len(offset_mapping) or\n",
    "                        end_index >= len(offset_mapping) or\n",
    "                        offset_mapping[start_index] is None or\n",
    "                        offset_mapping[end_index] is None):\n",
    "                        continue\n",
    "\n",
    "                    if (end_index < start_index or\n",
    "                        end_index - start_index + 1 > max_answer_length):\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    text = context[start_char:end_char]\n",
    "                    score = start_logits[start_index] + end_logits[end_index]\n",
    "\n",
    "                    if score > best_answer[\"score\"]:\n",
    "                        best_answer = {\"text\": text, \"score\": float(score)}\n",
    "\n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "471f9172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "√âvaluation du mod√®le RoBERTa\n",
      "================================================================================\n",
      "G√©n√©ration des pr√©dictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-traitement des pr√©dictions...\n",
      "\n",
      "================================================================================\n",
      "R√©sultats de l'√©valuation - RoBERTa\n",
      "================================================================================\n",
      "F1 Score: 91.96%\n",
      "Exact Match: 85.65%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âvaluation du mod√®le RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"G√©n√©ration des pr√©dictions...\")\n",
    "raw_predictions = trainer.predict(tokenized_validation)\n",
    "\n",
    "print(\"Post-traitement des pr√©dictions...\")\n",
    "final_predictions = postprocess_qa_predictions(\n",
    "    eval_dataset,\n",
    "    tokenized_validation,\n",
    "    raw_predictions.predictions\n",
    ")\n",
    "\n",
    "# Calcul des m√©triques SQuAD\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "formatted_predictions = [\n",
    "    {\"id\": k, \"prediction_text\": v}\n",
    "    for k, v in final_predictions.items()\n",
    "]\n",
    "references = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]}\n",
    "    for ex in eval_dataset\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=formatted_predictions, references=references)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R√©sultats de l'√©valuation - RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "print(f\"F1 Score: {results['f1']:.2f}%\")\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b27590e",
   "metadata": {},
   "source": [
    "## 13. Test d'Inf√©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63a30a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Test d'inf√©rence - RoBERTa\n",
      "================================================================================\n",
      "\n",
      "Question: How large is the Amazon basin?\n",
      "R√©ponse: 7,000,000 square kilometres\n",
      "Confiance: 0.9721\n",
      "Temps: 10.92 ms\n",
      "\n",
      "Question: What is another name for the Amazon rainforest?\n",
      "R√©ponse: Amazonia\n",
      "Confiance: 0.9851\n",
      "Temps: 4.17 ms\n",
      "\n",
      "Temps d'inf√©rence moyen: 7.54 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Test d'inf√©rence - RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=OUTPUT_DIR,\n",
    "    tokenizer=OUTPUT_DIR,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "test_context = \"\"\"\n",
    "The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical\n",
    "rainforest in the Amazon biome that covers most of the Amazon basin of South America.\n",
    "The basin is 7,000,000 square kilometres. The rainforest represents over half of the\n",
    "planet's remaining rainforests and comprises the largest and most biodiverse tract\n",
    "of tropical rainforest in the world.\n",
    "\"\"\"\n",
    "\n",
    "test_questions = [\n",
    "    \"How large is the Amazon basin?\",\n",
    "    \"What is another name for the Amazon rainforest?\",\n",
    "]\n",
    "\n",
    "inference_times = []\n",
    "\n",
    "for question in test_questions:\n",
    "    start = time.time()\n",
    "    result = qa_pipeline(question=question, context=test_context)\n",
    "    inference_time = (time.time() - start) * 1000\n",
    "    inference_times.append(inference_time)\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"R√©ponse: {result['answer']}\")\n",
    "    print(f\"Confiance: {result['score']:.4f}\")\n",
    "    print(f\"Temps: {inference_time:.2f} ms\")\n",
    "\n",
    "avg_inference_time = np.mean(inference_times)\n",
    "print(f\"\\nTemps d'inf√©rence moyen: {avg_inference_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4bedca",
   "metadata": {},
   "source": [
    "## 14. Sauvegarde des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba6b5786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats sauvegard√©s dans: ../models/roberta_squad_finetuned/results.json\n",
      "\n",
      "================================================================================\n",
      "R√©sum√© des r√©sultats - RoBERTa\n",
      "================================================================================\n",
      "model_name: roberta-base\n",
      "model_type: roberta\n",
      "finetuned: True\n",
      "f1: 91.95973256997178\n",
      "exact_match: 85.6480605487228\n",
      "training_time_minutes: 101.91337714592616\n",
      "avg_inference_time_ms: 7.544159889221191\n",
      "total_parameters: 124056578\n",
      "trainable_parameters: 124056578\n",
      "num_train_samples: 87599\n",
      "num_eval_samples: 10570\n",
      "num_epochs: 3\n",
      "batch_size: 32\n",
      "learning_rate: 3e-05\n",
      "max_length: 384\n",
      "doc_stride: 128\n"
     ]
    }
   ],
   "source": [
    "results_summary = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"model_type\": \"roberta\",\n",
    "    \"finetuned\": True,\n",
    "    \"f1\": results[\"f1\"],\n",
    "    \"exact_match\": results[\"exact_match\"],\n",
    "    \"training_time_minutes\": training_time / 60,\n",
    "    \"avg_inference_time_ms\": avg_inference_time,\n",
    "    \"total_parameters\": total_params,\n",
    "    \"trainable_parameters\": trainable_params,\n",
    "    \"num_train_samples\": len(train_dataset),\n",
    "    \"num_eval_samples\": len(eval_dataset),\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"doc_stride\": DOC_STRIDE,\n",
    "}\n",
    "\n",
    "output_path = f\"{OUTPUT_DIR}/results.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"R√©sultats sauvegard√©s dans: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R√©sum√© des r√©sultats - RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "for key, value in results_summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c93e5",
   "metadata": {},
   "source": [
    "## 15. Comparaison avec Baseline\n",
    "\n",
    "Comparaison des performances avant et apr√®s fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb49015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARAISON: Baseline vs Fine-tuned (RoBERTa)\n",
      "================================================================================\n",
      "M√©trique                         Baseline      Fine-tuned            Gain\n",
      "--------------------------------------------------------------------------------\n",
      "F1 Score (%)                         6.50           91.96          +85.46\n",
      "Exact Match (%)                      1.25           85.65          +84.40\n",
      "================================================================================\n",
      "\n",
      "üéØ Am√©lioration F1: +85.46 points gr√¢ce au fine-tuning!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Charger les r√©sultats baseline\n",
    "baseline_path = \"../models/roberta_baseline/results.json\"\n",
    "if os.path.exists(baseline_path):\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARAISON: Baseline vs Fine-tuned (RoBERTa)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'M√©trique':<25} {'Baseline':>15} {'Fine-tuned':>15} {'Gain':>15}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'F1 Score (%)':<25} {baseline_results['f1']:>15.2f} {results['f1']:>15.2f} {results['f1']-baseline_results['f1']:>+15.2f}\")\n",
    "    print(f\"{'Exact Match (%)':<25} {baseline_results['exact_match']:>15.2f} {results['exact_match']:>15.2f} {results['exact_match']-baseline_results['exact_match']:>+15.2f}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüéØ Am√©lioration F1: +{results['f1']-baseline_results['f1']:.2f} points gr√¢ce au fine-tuning!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Fichier baseline non trouv√©: {baseline_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
