{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64577868",
   "metadata": {},
   "source": [
    "# Notebook 1 : √âvaluation Baseline des Mod√®les Pr√©-entra√Æn√©s\n",
    "\n",
    "**Cours:** M2 Datascale - Fouille de Donn√©es  \n",
    "\n",
    "## Objectifs\n",
    "- √âvaluer 3 mod√®les pr√©-entra√Æn√©s **SANS fine-tuning** sur SQuAD v1.1\n",
    "- √âtablir une baseline de performance pour comparer avec les mod√®les fine-tun√©s\n",
    "- Calculer les m√©triques F1 Score et Exact Match\n",
    "- Mesurer le temps d'inf√©rence\n",
    "\n",
    "## Mod√®les √âvalu√©s\n",
    "1. **DistilBERT-base-uncased** (66M param√®tres)\n",
    "2. **RoBERTa-base** (125M param√®tres)\n",
    "3. **DeBERTa-v3-base** (184M param√®tres)\n",
    "\n",
    "## Hypoth√®se\n",
    "Les mod√®les pr√©-entra√Æn√©s sans fine-tuning devraient obtenir des F1 scores autour de 40-45%, d√©montrant l'importance du fine-tuning qui devrait am√©liorer les performances de ~40-45 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb71551",
   "metadata": {},
   "source": [
    "## 1. V√©rification de l'Environnement GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae2f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√©rification de l'environnement GPU...\n",
      "GPU d√©tect√©: NVIDIA GeForce RTX 5080\n",
      "M√©moire disponible: 16.60 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"V√©rification de l'environnement GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    device_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU d√©tect√©: {device_name}\")\n",
    "    print(f\"M√©moire disponible: {device_memory:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Aucun GPU d√©tect√©. Utilisation du CPU.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69027e",
   "metadata": {},
   "source": [
    "## 2. Installation des D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ac9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate accelerate torch sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f4243",
   "metadata": {},
   "source": [
    "## 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbae74d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khaledbouabdallah/Projects/qa-finetuning-squad-webapp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    DebertaV2Tokenizer,\n",
    "    pipeline\n",
    ")\n",
    "import evaluate\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7703d113",
   "metadata": {},
   "source": [
    "## 4. Configuration\n",
    "\n",
    "Nous allons √©valuer chaque mod√®le sur le validation set de SQuAD v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f10c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Configuration de l'√©valuation baseline\n",
      "================================================================================\n",
      "Mod√®les: ['distilbert', 'roberta', 'deberta']\n",
      "√âchantillons de validation: 10570\n",
      "Device: cuda\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Mod√®les √† √©valuer (pr√©-entra√Æn√©s, SANS fine-tuning)\n",
    "MODELS = {\n",
    "    \"distilbert\": \"distilbert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"deberta\": \"microsoft/deberta-v3-base\"\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "MAX_LENGTH = 384\n",
    "DOC_STRIDE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Taille du dataset de validation\n",
    "USE_FULL_DATASET = True\n",
    "MAX_EVAL_SAMPLES = 10570 if USE_FULL_DATASET else 1000\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Configuration de l'√©valuation baseline\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mod√®les: {list(MODELS.keys())}\")\n",
    "print(f\"√âchantillons de validation: {MAX_EVAL_SAMPLES}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c1771",
   "metadata": {},
   "source": [
    "## 5. Chargement du Dataset SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb09a63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset SQuAD v1.1...\n",
      "Nombre d'exemples: 10570\n",
      "Colonnes: ['id', 'title', 'context', 'question', 'answers']\n",
      "\n",
      "Exemple:\n",
      "{'id': '56be4db0acb8001400a502ec', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement du dataset SQuAD v1.1...\")\n",
    "squad = load_dataset(\"squad\", split=\"validation\")\n",
    "\n",
    "if not USE_FULL_DATASET:\n",
    "    squad = squad.select(range(MAX_EVAL_SAMPLES))\n",
    "\n",
    "print(f\"Nombre d'exemples: {len(squad)}\")\n",
    "print(f\"Colonnes: {squad.column_names}\")\n",
    "print(f\"\\nExemple:\")\n",
    "print(squad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b338a07a",
   "metadata": {},
   "source": [
    "## 6. Fonction d'√âvaluation\n",
    "\n",
    "Cette fonction √©value un mod√®le sans fine-tuning sur le dataset SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c061d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline_model(model_name, model_key):\n",
    "    \"\"\"\n",
    "    √âvalue un mod√®le baseline (pr√©-entra√Æn√© sans fine-tuning)\n",
    "\n",
    "    Args:\n",
    "        model_name: Nom du mod√®le HuggingFace\n",
    "        model_key: Cl√© courte pour sauvegarder les r√©sultats\n",
    "\n",
    "    Returns:\n",
    "        dict: R√©sultats (F1, EM, inference_time)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"√âvaluation de {model_name} (BASELINE - pas de fine-tuning)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Charger explicitement le tokenizer et le mod√®le\n",
    "    print(\"Chargement du mod√®le et tokenizer...\")\n",
    "    if \"deberta\" in model_name.lower():\n",
    "        tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    # D√©placer le mod√®le sur le GPU\n",
    "    if device == \"cuda\":\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Cr√©er le pipeline QA avec le tokenizer et mod√®le charg√©s\n",
    "    qa_pipeline = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if device == \"cuda\" else -1\n",
    "    )\n",
    "\n",
    "    # Pr√©parer les pr√©dictions\n",
    "    predictions = []\n",
    "    references = []\n",
    "    inference_times = []\n",
    "\n",
    "    print(f\"\\nInf√©rence sur {len(squad)} exemples...\")\n",
    "\n",
    "    for idx, example in enumerate(tqdm(squad)):\n",
    "        # Extraire les donn√©es\n",
    "        context = example['context']\n",
    "        question = example['question']\n",
    "        answer = example['answers']['text'][0]\n",
    "        answer_start = example['answers']['answer_start'][0]\n",
    "\n",
    "        # Faire la pr√©diction avec mesure du temps\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = qa_pipeline(question=question, context=context)\n",
    "            inference_time = time.time() - start_time\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            # Stocker les r√©sultats\n",
    "            predictions.append({\n",
    "                'id': example['id'],\n",
    "                'prediction_text': result['answer']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # En cas d'erreur, utiliser une r√©ponse vide\n",
    "            inference_time = time.time() - start_time\n",
    "            inference_times.append(inference_time)\n",
    "            predictions.append({\n",
    "                'id': example['id'],\n",
    "                'prediction_text': ''\n",
    "            })\n",
    "\n",
    "        references.append({\n",
    "            'id': example['id'],\n",
    "            'answers': example['answers']\n",
    "        })\n",
    "\n",
    "        # Afficher la progression\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            avg_time = np.mean(inference_times[-500:])\n",
    "            print(f\"  Trait√© {idx + 1}/{len(squad)} exemples (avg time: {avg_time:.3f}s)\")\n",
    "\n",
    "    # Calculer les m√©triques\n",
    "    print(\"\\nCalcul des m√©triques...\")\n",
    "    squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "    # Formater pour la m√©trique\n",
    "    formatted_predictions = [\n",
    "        {'id': p['id'], 'prediction_text': p['prediction_text']}\n",
    "        for p in predictions\n",
    "    ]\n",
    "    formatted_references = [\n",
    "        {'id': r['id'], 'answers': r['answers']}\n",
    "        for r in references\n",
    "    ]\n",
    "\n",
    "    metrics = squad_metric.compute(\n",
    "        predictions=formatted_predictions,\n",
    "        references=formatted_references\n",
    "    )\n",
    "\n",
    "    # Ajouter le temps d'inf√©rence\n",
    "    metrics['avg_inference_time'] = float(np.mean(inference_times))\n",
    "    metrics['total_inference_time'] = float(np.sum(inference_times))\n",
    "\n",
    "    # Afficher les r√©sultats\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"R√âSULTATS - {model_name} (BASELINE)\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"F1 Score: {metrics['f1']:.2f}%\")\n",
    "    print(f\"Exact Match: {metrics['exact_match']:.2f}%\")\n",
    "    print(f\"Temps moyen par exemple: {metrics['avg_inference_time']:.3f}s\")\n",
    "    print(f\"Temps total: {metrics['total_inference_time']:.2f}s\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # Sauvegarder les r√©sultats\n",
    "    output_dir = f\"../models/{model_key}_baseline\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results_file = os.path.join(output_dir, \"results.json\")\n",
    "    results_to_save = {\n",
    "        \"model_name\": model_name,\n",
    "        \"model_type\": model_key,\n",
    "        \"finetuned\": False,\n",
    "        \"f1\": metrics['f1'],\n",
    "        \"exact_match\": metrics['exact_match'],\n",
    "        \"avg_inference_time\": metrics['avg_inference_time'],\n",
    "        \"total_inference_time\": metrics['total_inference_time'],\n",
    "        \"num_eval_samples\": len(squad)\n",
    "    }\n",
    "\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úì R√©sultats sauvegard√©s dans: {results_file}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8565bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distilbert': 'distilbert-base-uncased',\n",
       " 'roberta': 'roberta-base',\n",
       " 'deberta': 'microsoft/deberta-v3-base'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d6cd7",
   "metadata": {},
   "source": [
    "## 7. √âvaluation de Tous les Mod√®les Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8eca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "√âvaluation de microsoft/deberta-v3-base (BASELINE - pas de fine-tuning)\n",
      "================================================================================\n",
      "Chargement du mod√®le et tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inf√©rence sur 10570 exemples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/10570 [00:02<25:38,  6.87it/s]  You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  5%|‚ñç         | 500/10570 [00:24<07:25, 22.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 500/10570 exemples (avg time: 0.049s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 998/10570 [00:48<07:13, 22.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 1000/10570 exemples (avg time: 0.046s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 1499/10570 [01:11<06:55, 21.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 1500/10570 exemples (avg time: 0.046s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 1999/10570 [01:34<06:50, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 2000/10570 exemples (avg time: 0.046s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñé       | 2500/10570 [01:57<06:06, 22.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 2500/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 2998/10570 [02:19<05:28, 23.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 3000/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 3499/10570 [02:42<05:16, 22.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 3500/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 4000/10570 [03:05<04:52, 22.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 4000/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 4498/10570 [03:30<04:28, 22.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 4500/10570 exemples (avg time: 0.050s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 4999/10570 [03:53<04:21, 21.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 5000/10570 exemples (avg time: 0.046s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 5499/10570 [04:16<03:57, 21.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 5500/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 5999/10570 [04:39<03:30, 21.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 6000/10570 exemples (avg time: 0.046s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 6499/10570 [05:02<03:05, 21.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 6500/10570 exemples (avg time: 0.046s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 6999/10570 [05:25<02:37, 22.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 7000/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7498/10570 [05:47<02:17, 22.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 7500/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 8000/10570 [06:10<01:59, 21.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 8000/10570 exemples (avg time: 0.046s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8498/10570 [06:33<01:34, 21.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 8500/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 8999/10570 [06:56<01:09, 22.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 9000/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 9500/10570 [07:19<00:48, 22.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 9500/10570 exemples (avg time: 0.045s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 9998/10570 [07:42<00:27, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 10000/10570 exemples (avg time: 0.046s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 10498/10570 [08:06<00:03, 21.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trait√© 10500/10570 exemples (avg time: 0.047s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10570/10570 [08:09<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calcul des m√©triques...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "R√âSULTATS - microsoft/deberta-v3-base (BASELINE)\n",
      "--------------------------------------------------------------------------------\n",
      "F1 Score: 5.95%\n",
      "Exact Match: 0.20%\n",
      "Temps moyen par exemple: 0.046s\n",
      "Temps total: 485.39s\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úì R√©sultats sauvegard√©s dans: ../models/deberta_baseline/results.json\n",
      "\n",
      "================================================================================\n",
      "√âVALUATION BASELINE TERMIN√âE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Stocker tous les r√©sultats\n",
    "all_results = {}\n",
    "\n",
    "for model_key, model_name in MODELS.items():\n",
    "    try:\n",
    "        metrics = evaluate_baseline_model(model_name, model_key)\n",
    "        all_results[model_key] = metrics\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur lors de l'√©valuation de {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√âVALUATION BASELINE TERMIN√âE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff7fdd",
   "metadata": {},
   "source": [
    "## 8. Tableau R√©capitulatif des Performances Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4bd0b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "R√âSUM√â DES PERFORMANCES BASELINE (pr√©-entra√Æn√©s sans fine-tuning)\n",
      "================================================================================\n",
      "    Mod√®le F1 (%) EM (%) Temps/exemple (s)\n",
      "DISTILBERT   6.82   0.40             0.004\n",
      "   ROBERTA   6.50   1.25             0.006\n",
      "   DEBERTA   5.95   0.20             0.046\n",
      "================================================================================\n",
      "\n",
      "üí° Ces scores baseline serviront de r√©f√©rence pour mesurer\n",
      "   l'am√©lioration apport√©e par le fine-tuning sur SQuAD.\n",
      "\n",
      "üìä Am√©lioration attendue apr√®s fine-tuning: +40-45 points de F1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les r√©sultats depuis les fichiers JSON sauvegard√©s\n",
    "summary_data = []\n",
    "\n",
    "for model_key in MODELS.keys():\n",
    "    results_file = f\"../models/{model_key}_baseline/results.json\"\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        summary_data.append({\n",
    "            'Mod√®le': model_key.upper(),\n",
    "            'F1 (%)': f\"{metrics['f1']:.2f}\",\n",
    "            'EM (%)': f\"{metrics['exact_match']:.2f}\",\n",
    "            'Temps/exemple (s)': f\"{metrics['avg_inference_time']:.3f}\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Fichier non trouv√©: {results_file}\")\n",
    "\n",
    "if summary_data:\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"R√âSUM√â DES PERFORMANCES BASELINE (pr√©-entra√Æn√©s sans fine-tuning)\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_summary.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\nüí° Ces scores baseline serviront de r√©f√©rence pour mesurer\")\n",
    "    print(\"   l'am√©lioration apport√©e par le fine-tuning sur SQuAD.\")\n",
    "    print(\"\\nüìä Am√©lioration attendue apr√®s fine-tuning: +40-45 points de F1\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun r√©sultat disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af761227",
   "metadata": {},
   "source": [
    "\n",
    "Les r√©sultats obtenus mettent en √©vidence des diff√©rences notables entre les mod√®les √©valu√©s, tant en termes de performance que de co√ªt computationnel.\n",
    "\n",
    "Le mod√®le DistilBERT, bien que con√ßu pour √™tre l√©ger et rapide, pr√©sente les scores Exact Match et F1 les plus faibles. Ce comportement est attendu, car la distillation r√©duit la capacit√© du mod√®le √† capturer des d√©pendances contextuelles complexes. En revanche, son temps d‚Äôinf√©rence moyen est le plus faible, ce qui en fait un candidat pertinent pour des applications n√©cessitant des contraintes fortes de latence.\n",
    "\n",
    "RoBERTa-base obtient des performances interm√©diaires, avec une am√©lioration nette des scores par rapport √† DistilBERT. Cette progression s‚Äôexplique par une capacit√© de repr√©sentation plus √©lev√©e et un pr√©-entra√Ænement plus riche. Toutefois, cette am√©lioration se fait au prix d‚Äôun temps d‚Äôinf√©rence plus important, illustrant un compromis classique entre performance et complexit√© du mod√®le.\n",
    "\n",
    "DeBERTa-v3-base se distingue par les meilleurs r√©sultats en Exact Match et en F1-score. Sa sup√©riorit√© peut √™tre attribu√©e √† son architecture am√©lior√©e, notamment l‚Äôutilisation de m√©canismes de d√©sentrelacement des repr√©sentations de contenu et de position, favorisant une compr√©hension plus fine du contexte. N√©anmoins, ce gain de performance s‚Äôaccompagne d‚Äôun co√ªt computationnel plus √©lev√©, comme en t√©moigne son temps d‚Äôinf√©rence moyen.\n",
    "\n",
    "Globalement, ces r√©sultats confirment l‚Äôexistence d‚Äôun compromis clair entre pr√©cision et efficacit√© computationnelle. Ils justifient la poursuite du travail avec une phase de fine-tuning supervis√©, dont l‚Äôobjectif sera d‚Äôam√©liorer les performances tout en analysant l‚Äôimpact de l‚Äôentra√Ænement sur ce compromis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468719a",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "Les mod√®les baseline (pr√©-entra√Æn√©s sans fine-tuning) ont √©t√© √©valu√©s. Ces r√©sultats serviront de point de comparaison pour d√©montrer l'efficacit√© du fine-tuning.\n",
    "\n",
    "**Prochaines √©tapes:**\n",
    "1. Fine-tuner chaque mod√®le sur SQuAD (notebooks 02, 03, 04)\n",
    "2. Comparer les performances baseline vs fine-tuned (notebook 05)\n",
    "3. Analyser le gain de performance apport√© par le fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
