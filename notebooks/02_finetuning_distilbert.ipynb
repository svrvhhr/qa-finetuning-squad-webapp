{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "notebook-header",
      "metadata": {
        "id": "notebook-header"
      },
      "source": [
        "# Notebook 2 : Fine-tuning de DistilBERT pour Question-Answering\n",
        "\n",
        "**Cours:** M2 Datascale - Fouille de Données  \n",
        "\n",
        "## Objectifs\n",
        "- Fine-tuner le modèle DistilBERT-base-uncased sur SQuAD v1.1\n",
        "- Évaluer avec les métriques F1 Score et Exact Match\n",
        "- Mesurer le temps d'inférence\n",
        "- Sauvegarder le modèle fine-tuné\n",
        "\n",
        "## Modèle\n",
        "- **Architecture:** DistilBERT (Sanh et al., 2019)\n",
        "- **Paramètres:** 66M\n",
        "- **Caractéristiques:** Version distillée de BERT, 40% plus rapide\n",
        "\n",
        "## Références\n",
        "- Sanh et al. (2019). \"DistilBERT, a distilled version of BERT\"\n",
        "- Rajpurkar et al. (2016). \"SQuAD: 100,000+ Questions for Machine Comprehension\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "## 1. Vérification de l'Environnement GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "check-gpu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-gpu",
        "outputId": "7a32e521-a207-4197-e057-4be1ec0a15d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vérification de l'environnement GPU...\n",
            "GPU détecté: NVIDIA GeForce RTX 5080\n",
            "Mémoire disponible: 16.60 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"Vérification de l'environnement GPU...\")\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    device_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU détecté: {device_name}\")\n",
        "    print(f\"Mémoire disponible: {device_memory:.2f} GB\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"Aucun GPU détecté. Utilisation du CPU.\")\n",
        "    print(\"Note: L'entraînement sur CPU sera significativement plus lent.\")\n",
        "    device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "## 2. Installation des Dépendances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "install",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install",
        "outputId": "1900a384-7289-4766-a72c-907862df9e6a"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate accelerate torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "## 3. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "imports",
      "metadata": {
        "id": "imports"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/khaledbouabdallah/Projects/qa-finetuning-squad-webapp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DefaultDataCollator,\n",
        "    pipeline\n",
        ")\n",
        "import evaluate\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "## 4. Configuration des Hyperparamètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "config",
      "metadata": {
        "id": "config"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Configuration des hyperparamètres\n",
            "================================================================================\n",
            "Modèle: distilbert-base-uncased\n",
            "Longueur maximale: 384 tokens\n",
            "Stride: 128 tokens\n",
            "Learning rate: 3e-05\n",
            "Nombre d'epochs: 3\n",
            "Batch size: 64\n",
            "Échantillons entraînement: 87599\n",
            "Échantillons validation: 10570\n"
          ]
        }
      ],
      "source": [
        "# Seed pour la reproductibilité\n",
        "SEED = 42\n",
        "\n",
        "# Configuration du modèle\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "OUTPUT_DIR = \"../models/distilbert_squad_finetuned\"\n",
        "\n",
        "# Hyperparamètres de tokenisation\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 128\n",
        "\n",
        "# Hyperparamètres d'entraînement\n",
        "LEARNING_RATE = 3e-5\n",
        "NUM_EPOCHS = 3\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "EVAL_BATCH_SIZE = 64\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "# Taille du dataset\n",
        "USE_FULL_DATASET = True  # Mettre False pour test rapide\n",
        "MAX_TRAIN_SAMPLES = 87599 if USE_FULL_DATASET else 5000\n",
        "MAX_EVAL_SAMPLES = 10570 if USE_FULL_DATASET else 1000\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Configuration des hyperparamètres\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Modèle: {MODEL_NAME}\")\n",
        "print(f\"Longueur maximale: {MAX_LENGTH} tokens\")\n",
        "print(f\"Stride: {DOC_STRIDE} tokens\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Nombre d'epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch size: {TRAIN_BATCH_SIZE}\")\n",
        "print(f\"Échantillons entraînement: {MAX_TRAIN_SAMPLES}\")\n",
        "print(f\"Échantillons validation: {MAX_EVAL_SAMPLES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "## 5. Fixation du Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "seed",
      "metadata": {
        "id": "seed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed fixé à 42 pour la reproductibilité.\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Fixe le seed pour assurer la reproductibilité.\n",
        "\n",
        "    Args:\n",
        "        seed (int): Valeur du seed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"Seed fixé à {SEED} pour la reproductibilité.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "## 6. Chargement des Données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "load-data",
      "metadata": {
        "id": "load-data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Chargement du dataset SQuAD v1.1\n",
            "================================================================================\n",
            "Échantillons d'entraînement: 87599\n",
            "Échantillons de validation: 10570\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Chargement du dataset SQuAD v1.1\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "squad = load_dataset(\"squad\")\n",
        "\n",
        "# Sélection des sous-ensembles\n",
        "train_dataset = squad[\"train\"].shuffle(seed=SEED).select(\n",
        "    range(min(MAX_TRAIN_SAMPLES, len(squad[\"train\"])))\n",
        ")\n",
        "eval_dataset = squad[\"validation\"].shuffle(seed=SEED).select(\n",
        "    range(min(MAX_EVAL_SAMPLES, len(squad[\"validation\"])))\n",
        ")\n",
        "\n",
        "print(f\"Échantillons d'entraînement: {len(train_dataset)}\")\n",
        "print(f\"Échantillons de validation: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "## 7. Chargement du Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "tokenizer",
      "metadata": {
        "id": "tokenizer"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Chargement du tokenizer\n",
            "================================================================================\n",
            "Tokenizer chargé: distilbert-base-uncased\n",
            "Taille du vocabulaire: 30522\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Chargement du tokenizer\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print(f\"Tokenizer chargé: {MODEL_NAME}\")\n",
        "print(f\"Taille du vocabulaire: {tokenizer.vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-8",
      "metadata": {
        "id": "section-8"
      },
      "source": [
        "## 8. Préparation des Données (Tokenisation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "preprocessing",
      "metadata": {
        "id": "preprocessing"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        "    \"\"\"\n",
        "    Tokenise les exemples d'entraînement et aligne les positions des réponses.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): Batch d'exemples contenant questions, contextes et réponses\n",
        "\n",
        "    Returns:\n",
        "        dict: Features tokenisées avec positions start/end\n",
        "    \"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "            continue\n",
        "\n",
        "        start_char = answers[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        if not (offsets[token_start_index][0] <= start_char and\n",
        "                offsets[token_end_index][1] >= end_char):\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            start_positions.append(token_start_index - 1)\n",
        "\n",
        "            while offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            end_positions.append(token_end_index + 1)\n",
        "\n",
        "    tokenized[\"start_positions\"] = start_positions\n",
        "    tokenized[\"end_positions\"] = end_positions\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "def prepare_validation_features(examples):\n",
        "    \"\"\"\n",
        "    Tokenise les exemples de validation en conservant les métadonnées.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): Batch d'exemples de validation\n",
        "\n",
        "    Returns:\n",
        "        dict: Features tokenisées avec IDs d'exemples et offset mapping\n",
        "    \"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
        "    tokenized[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized[\"input_ids\"])):\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "        context_index = 1\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        tokenized[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "apply-preprocessing",
      "metadata": {
        "id": "apply-preprocessing"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Tokenisation des données\n",
            "================================================================================\n",
            "Tokenisation de l'ensemble d'entraînement...\n",
            "Tokenisation de l'ensemble de validation...\n",
            "\n",
            "Features d'entraînement: 88524\n",
            "Features de validation: 10784\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Tokenisation des données\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Tokenisation de l'ensemble d'entraînement...\")\n",
        "tokenized_train = train_dataset.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "print(\"Tokenisation de l'ensemble de validation...\")\n",
        "tokenized_validation = eval_dataset.map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        ")\n",
        "\n",
        "# Validation set pour eval_loss (doit contenir start_positions / end_positions)\n",
        "tokenized_validation_for_loss = eval_dataset.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        ")\n",
        "\n",
        "print(f\"\\nFeatures d'entraînement: {len(tokenized_train)}\")\n",
        "print(f\"Features de validation: {len(tokenized_validation)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-9",
      "metadata": {
        "id": "section-9"
      },
      "source": [
        "## 9. Chargement du Modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "load-model",
      "metadata": {
        "id": "load-model"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Initialisation du modèle\n",
            "================================================================================\n",
            "Modèle: distilbert-base-uncased\n",
            "Paramètres totaux: 66,364,418\n",
            "Paramètres entraînables: 66,364,418\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Initialisation du modèle\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Modèle: {MODEL_NAME}\")\n",
        "print(f\"Paramètres totaux: {total_params:,}\")\n",
        "print(f\"Paramètres entraînables: {trainable_params:,}\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-10",
      "metadata": {
        "id": "section-10"
      },
      "source": [
        "## 10. Configuration de l'Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "training-setup",
      "metadata": {
        "id": "training-setup"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer configuré avec succès.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../models/results_distilbert_squad\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=3e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train, # Correction: Utiliser le dataset tokenisé pour l'entraînement\n",
        "    eval_dataset=tokenized_validation_for_loss,\n",
        "    processing_class=tokenizer, # Adressé le FutureWarning ici\n",
        "    data_collator=DefaultDataCollator(),\n",
        ")\n",
        "\n",
        "print(\"Trainer configuré avec succès.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-11",
      "metadata": {
        "id": "section-11"
      },
      "source": [
        "## 11. Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "training",
      "metadata": {
        "id": "training"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Début de l'entraînement\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4152/4152 55:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.306900</td>\n",
              "      <td>1.243359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.040000</td>\n",
              "      <td>1.150378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.841500</td>\n",
              "      <td>1.155398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Entraînement terminé\n",
            "================================================================================\n",
            "Durée totale: 55.62 minutes\n",
            "Loss finale: 1.3449\n",
            "\n",
            "Modèle sauvegardé dans: ../models/distilbert_squad_finetuned\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Début de l'entraînement\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = time.time()\n",
        "train_result = trainer.train()\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Entraînement terminé\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Durée totale: {training_time/60:.2f} minutes\")\n",
        "print(f\"Loss finale: {train_result.training_loss:.4f}\")\n",
        "\n",
        "# Sauvegarde\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"\\nModèle sauvegardé dans: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-12",
      "metadata": {
        "id": "section-12"
      },
      "source": [
        "## 12. Évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "postprocessing",
      "metadata": {
        "id": "postprocessing"
      },
      "outputs": [],
      "source": [
        "def postprocess_qa_predictions(examples, features, raw_predictions,\n",
        "                                n_best=20, max_answer_length=30):\n",
        "    \"\"\"\n",
        "    Post-traite les prédictions brutes pour extraire les réponses textuelles.\n",
        "\n",
        "    Args:\n",
        "        examples: Exemples originaux\n",
        "        features: Features tokenisées\n",
        "        raw_predictions: Logits de début et fin\n",
        "        n_best: Nombre de candidats à considérer\n",
        "        max_answer_length: Longueur maximale de la réponse\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping ID -> texte de réponse\n",
        "    \"\"\"\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    predictions = {}\n",
        "\n",
        "    for example_index, example in enumerate(examples):\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        context = example[\"context\"]\n",
        "\n",
        "        best_answer = {\"text\": \"\", \"score\": -float(\"inf\")}\n",
        "\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            start_indexes = np.argsort(start_logits)[-n_best:][::-1]\n",
        "            end_indexes = np.argsort(end_logits)[-n_best:][::-1]\n",
        "\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if (start_index >= len(offset_mapping) or\n",
        "                        end_index >= len(offset_mapping) or\n",
        "                        offset_mapping[start_index] is None or\n",
        "                        offset_mapping[end_index] is None):\n",
        "                        continue\n",
        "\n",
        "                    if (end_index < start_index or\n",
        "                        end_index - start_index + 1 > max_answer_length):\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    text = context[start_char:end_char]\n",
        "                    score = start_logits[start_index] + end_logits[end_index]\n",
        "\n",
        "                    if score > best_answer[\"score\"]:\n",
        "                        best_answer = {\"text\": text, \"score\": float(score)}\n",
        "\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "evaluation",
      "metadata": {
        "id": "evaluation"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Évaluation du modèle\n",
            "================================================================================\n",
            "Génération des prédictions...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-traitement des prédictions...\n",
            "\n",
            "================================================================================\n",
            "Résultats de l'évaluation\n",
            "================================================================================\n",
            "F1 Score: 84.41%\n",
            "Exact Match: 75.81%\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Évaluation du modèle\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Génération des prédictions...\")\n",
        "raw_predictions = trainer.predict(tokenized_validation)\n",
        "\n",
        "print(\"Post-traitement des prédictions...\")\n",
        "final_predictions = postprocess_qa_predictions(\n",
        "    eval_dataset,\n",
        "    tokenized_validation,\n",
        "    raw_predictions.predictions\n",
        ")\n",
        "\n",
        "# Calcul des métriques SQuAD\n",
        "metric = evaluate.load(\"squad\")\n",
        "\n",
        "formatted_predictions = [\n",
        "    {\"id\": k, \"prediction_text\": v}\n",
        "    for k, v in final_predictions.items()\n",
        "]\n",
        "references = [\n",
        "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]}\n",
        "    for ex in eval_dataset\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=formatted_predictions, references=references)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Résultats de l'évaluation\")\n",
        "print(\"=\"*80)\n",
        "print(f\"F1 Score: {results['f1']:.2f}%\")\n",
        "print(f\"Exact Match: {results['exact_match']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-13",
      "metadata": {
        "id": "section-13"
      },
      "source": [
        "## 13. Test d'Inférence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "inference",
      "metadata": {
        "id": "inference"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Test d'inférence\n",
            "================================================================================\n",
            "\n",
            "Question: How large is the Amazon basin?\n",
            "Réponse: 7,000,000 square kilometres\n",
            "Confiance: 0.9223\n",
            "Temps: 3.56 ms\n",
            "\n",
            "Question: What is another name for the Amazon rainforest?\n",
            "Réponse: Amazonia\n",
            "Confiance: 0.9450\n",
            "Temps: 2.87 ms\n",
            "\n",
            "Temps d'inférence moyen: 3.21 ms\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Test d'inférence\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=OUTPUT_DIR,\n",
        "    tokenizer=OUTPUT_DIR,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "test_context = \"\"\"\n",
        "The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical\n",
        "rainforest in the Amazon biome that covers most of the Amazon basin of South America.\n",
        "The basin is 7,000,000 square kilometres. The rainforest represents over half of the\n",
        "planet's remaining rainforests and comprises the largest and most biodiverse tract\n",
        "of tropical rainforest in the world.\n",
        "\"\"\"\n",
        "\n",
        "test_questions = [\n",
        "    \"How large is the Amazon basin?\",\n",
        "    \"What is another name for the Amazon rainforest?\",\n",
        "]\n",
        "\n",
        "inference_times = []\n",
        "\n",
        "for question in test_questions:\n",
        "    start = time.time()\n",
        "    result = qa_pipeline(question=question, context=test_context)\n",
        "    inference_time = (time.time() - start) * 1000\n",
        "    inference_times.append(inference_time)\n",
        "\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    print(f\"Réponse: {result['answer']}\")\n",
        "    print(f\"Confiance: {result['score']:.4f}\")\n",
        "    print(f\"Temps: {inference_time:.2f} ms\")\n",
        "\n",
        "avg_inference_time = np.mean(inference_times)\n",
        "print(f\"\\nTemps d'inférence moyen: {avg_inference_time:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-14",
      "metadata": {
        "id": "section-14"
      },
      "source": [
        "## 14. Sauvegarde des Résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "save-results",
      "metadata": {
        "id": "save-results"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultats sauvegardés dans: ../models/distilbert_squad_finetuned/results.json\n",
            "\n",
            "================================================================================\n",
            "Résumé des résultats - DistilBERT\n",
            "================================================================================\n",
            "model_name: distilbert-base-uncased\n",
            "model_type: distilbert\n",
            "finetuned: True\n",
            "f1: 84.41070434105153\n",
            "exact_match: 75.80889309366131\n",
            "training_time_minutes: 55.61639648278554\n",
            "avg_inference_time_ms: 3.21042537689209\n",
            "total_parameters: 66364418\n",
            "trainable_parameters: 66364418\n",
            "num_train_samples: 87599\n",
            "num_eval_samples: 10570\n",
            "num_epochs: 3\n",
            "batch_size: 64\n",
            "learning_rate: 3e-05\n",
            "max_length: 384\n",
            "doc_stride: 128\n"
          ]
        }
      ],
      "source": [
        "results_summary = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"model_type\": \"distilbert\",\n",
        "    \"finetuned\": True,\n",
        "    \"f1\": results[\"f1\"],\n",
        "    \"exact_match\": results[\"exact_match\"],\n",
        "    \"training_time_minutes\": training_time / 60,\n",
        "    \"avg_inference_time_ms\": avg_inference_time,\n",
        "    \"total_parameters\": total_params,\n",
        "    \"trainable_parameters\": trainable_params,\n",
        "    \"num_train_samples\": len(train_dataset),\n",
        "    \"num_eval_samples\": len(eval_dataset),\n",
        "    \"num_epochs\": NUM_EPOCHS,\n",
        "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"max_length\": MAX_LENGTH,\n",
        "    \"doc_stride\": DOC_STRIDE,\n",
        "}\n",
        "\n",
        "output_path = f\"{OUTPUT_DIR}/results.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(f\"Résultats sauvegardés dans: {output_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Résumé des résultats - DistilBERT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for key, value in results_summary.items():    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-15",
      "metadata": {
        "id": "section-15"
      },
      "source": [
        "## 15. Téléchargement (Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download",
      "metadata": {
        "id": "download"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exécution en environnement local.\n",
            "Résultats disponibles dans: ../models/distilbert_squad_finetuned/results.json\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_path)\n",
        "    print(f\"Fichier téléchargé: {output_path}\")\n",
        "except ImportError:\n",
        "    print(\"Exécution en environnement local.\")\n",
        "    print(f\"Résultats disponibles dans: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
