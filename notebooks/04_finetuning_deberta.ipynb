{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954b3f66",
   "metadata": {},
   "source": [
    "# Notebook 4 : Fine-tuning de DeBERTa pour Question-Answering\n",
    "\n",
    "**Cours:** M2 Datascale - Fouille de Donn√©es  \n",
    "\n",
    "## Objectifs\n",
    "- Fine-tuner le mod√®le DeBERTa-v3-base sur SQuAD v1.1\n",
    "- √âvaluer avec les m√©triques F1 Score et Exact Match\n",
    "- Mesurer le temps d'inf√©rence\n",
    "- Comparer avec DistilBERT et RoBERTa\n",
    "\n",
    "## Mod√®le\n",
    "- **Architecture:** DeBERTa-v3 (He et al., 2021)\n",
    "- **Param√®tres:** 184M\n",
    "- **Caract√©ristiques:** Disentangled attention mechanism, meilleure performance que RoBERTa\n",
    "\n",
    "## R√©f√©rences\n",
    "- He et al. (2021). \"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training\"\n",
    "- Rajpurkar et al. (2016). \"SQuAD: 100,000+ Questions for Machine Comprehension\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeba7c7",
   "metadata": {},
   "source": [
    "## 1. V√©rification de l'Environnement GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4ba288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√©rification de l'environnement GPU...\n",
      "GPU d√©tect√©: NVIDIA GeForce RTX 5080\n",
      "M√©moire disponible: 16.60 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"V√©rification de l'environnement GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    device_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU d√©tect√©: {device_name}\")\n",
    "    print(f\"M√©moire disponible: {device_memory:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Aucun GPU d√©tect√©. Utilisation du CPU.\")\n",
    "    print(\"Note: L'entra√Ænement sur CPU sera significativement plus lent.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93e4d1",
   "metadata": {},
   "source": [
    "## 2. Installation des D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01ed2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate accelerate torch sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436c667",
   "metadata": {},
   "source": [
    "## 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae933d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DebertaV2Tokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DefaultDataCollator,\n",
    "    pipeline\n",
    ")\n",
    "import evaluate\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c850c61",
   "metadata": {},
   "source": [
    "## 4. Configuration des Hyperparam√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eb8e67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Configuration des hyperparam√®tres - DeBERTa\n",
      "================================================================================\n",
      "Mod√®le: microsoft/deberta-v3-base\n",
      "Longueur maximale: 384 tokens\n",
      "Stride: 128 tokens\n",
      "Learning rate: 3e-05\n",
      "Nombre d'epochs: 3\n",
      "Batch size: 16\n",
      "√âchantillons entra√Ænement: 87599\n",
      "√âchantillons validation: 10570\n"
     ]
    }
   ],
   "source": [
    "# Seed pour la reproductibilit√©\n",
    "SEED = 42\n",
    "\n",
    "# Configuration du mod√®le\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "OUTPUT_DIR = \"../models/deberta_squad_finetuned\"\n",
    "\n",
    "# Hyperparam√®tres de tokenisation\n",
    "MAX_LENGTH = 384\n",
    "DOC_STRIDE = 128\n",
    "\n",
    "# Hyperparam√®tres d'entra√Ænement\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_EPOCHS = 3\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Taille du dataset\n",
    "USE_FULL_DATASET = True  # Mettre False pour test rapide\n",
    "MAX_TRAIN_SAMPLES = 87599 if USE_FULL_DATASET else 5000\n",
    "MAX_EVAL_SAMPLES = 10570 if USE_FULL_DATASET else 1000\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Configuration des hyperparam√®tres - DeBERTa\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mod√®le: {MODEL_NAME}\")\n",
    "print(f\"Longueur maximale: {MAX_LENGTH} tokens\")\n",
    "print(f\"Stride: {DOC_STRIDE} tokens\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Nombre d'epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"√âchantillons entra√Ænement: {MAX_TRAIN_SAMPLES}\")\n",
    "print(f\"√âchantillons validation: {MAX_EVAL_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e7260",
   "metadata": {},
   "source": [
    "## 5. Fixation du Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c03f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed fix√© √† 42 pour la reproductibilit√©.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Fixe le seed pour assurer la reproductibilit√©.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Valeur du seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Seed fix√© √† {SEED} pour la reproductibilit√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c61bc",
   "metadata": {},
   "source": [
    "## 6. Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f589555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Chargement du dataset SQuAD v1.1\n",
      "================================================================================\n",
      "√âchantillons d'entra√Ænement: 87599\n",
      "√âchantillons de validation: 10570\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Chargement du dataset SQuAD v1.1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# S√©lection des sous-ensembles\n",
    "train_dataset = squad[\"train\"].shuffle(seed=SEED).select(\n",
    "    range(min(MAX_TRAIN_SAMPLES, len(squad[\"train\"])))\n",
    ")\n",
    "eval_dataset = squad[\"validation\"].shuffle(seed=SEED).select(\n",
    "    range(min(MAX_EVAL_SAMPLES, len(squad[\"validation\"])))\n",
    ")\n",
    "\n",
    "print(f\"√âchantillons d'entra√Ænement: {len(train_dataset)}\")\n",
    "print(f\"√âchantillons de validation: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437bf47",
   "metadata": {},
   "source": [
    "## 7. Chargement du Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "912b52f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Chargement du tokenizer DeBERTa\n",
      "================================================================================\n",
      "Tokenizer charg√©: microsoft/deberta-v3-base\n",
      "Type de tokenizer: DebertaV2TokenizerFast\n",
      "Is fast: True\n",
      "Taille du vocabulaire: 128000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Chargement du tokenizer DeBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use fast tokenizer (required for QA training with offset mapping)\n",
    "# Note: This is the official HuggingFace approach for QA fine-tuning\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "print(f\"Tokenizer charg√©: {MODEL_NAME}\")\n",
    "print(f\"Type de tokenizer: {type(tokenizer).__name__}\")\n",
    "print(f\"Is fast: {tokenizer.is_fast}\")\n",
    "print(f\"Taille du vocabulaire: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7720f2",
   "metadata": {},
   "source": [
    "## 8. Pr√©paration des Donn√©es (Tokenisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "664b9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    \"\"\"\n",
    "    Tokenise les exemples d'entra√Ænement et aligne les positions des r√©ponses.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): Batch d'exemples contenant questions, contextes et r√©ponses\n",
    "\n",
    "    Returns:\n",
    "        dict: Features tokenis√©es avec positions start/end\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "\n",
    "        start_char = answers[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        if not (offsets[token_start_index][0] <= start_char and\n",
    "                offsets[token_end_index][1] >= end_char):\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    \"\"\"\n",
    "    Tokenise les exemples de validation en conservant les m√©tadonn√©es.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): Batch d'exemples de validation\n",
    "\n",
    "    Returns:\n",
    "        dict: Features tokenis√©es avec IDs d'exemples et offset mapping\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized[\"input_ids\"])):\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        tokenized[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8acf316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Tokenisation des donn√©es\n",
      "================================================================================\n",
      "Tokenisation de l'ensemble d'entra√Ænement...\n",
      "Tokenisation de l'ensemble de validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10570/10570 [00:01<00:00, 7019.47 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features d'entra√Ænement: 88316\n",
      "Features de validation: 10756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Tokenisation des donn√©es\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Tokenisation de l'ensemble d'entra√Ænement...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(\"Tokenisation de l'ensemble de validation...\")\n",
    "tokenized_validation = eval_dataset.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")\n",
    "\n",
    "# Validation set pour eval_loss (doit contenir start_positions / end_positions)\n",
    "tokenized_validation_for_loss = eval_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures d'entra√Ænement: {len(tokenized_train)}\")\n",
    "print(f\"Features de validation: {len(tokenized_validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a33aeb",
   "metadata": {},
   "source": [
    "## 9. Chargement du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a19e0401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Initialisation du mod√®le DeBERTa\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le: microsoft/deberta-v3-base\n",
      "Param√®tres totaux: 183,833,090\n",
      "Param√®tres entra√Ænables: 183,833,090\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Initialisation du mod√®le DeBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Mod√®le: {MODEL_NAME}\")\n",
    "print(f\"Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e8c2e",
   "metadata": {},
   "source": [
    "## 10. Configuration de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96186b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer DeBERTa configur√© avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/results_deberta_squad\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation_for_loss,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DefaultDataCollator(),\n",
    ")\n",
    "\n",
    "print(\"Trainer DeBERTa configur√© avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a250ea0",
   "metadata": {},
   "source": [
    "## 11. Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3ece3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "D√©but de l'entra√Ænement - DeBERTa\n",
      "================================================================================\n",
      "‚è±Ô∏è  Temps estim√©: ~90 minutes sur RTX 5080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16560' max='16560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16560/16560 2:13:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.782100</td>\n",
       "      <td>0.781998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.785151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.848480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Entra√Ænement termin√©\n",
      "================================================================================\n",
      "Dur√©e totale: 133.79 minutes\n",
      "Loss finale: 0.7403\n",
      "\n",
      "Mod√®le sauvegard√© dans: ../models/deberta_squad_finetuned\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"D√©but de l'entra√Ænement - DeBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "train_result = trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Entra√Ænement termin√©\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dur√©e totale: {training_time/60:.2f} minutes\")\n",
    "print(f\"Loss finale: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Sauvegarde\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"\\nMod√®le sauvegard√© dans: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc424a2",
   "metadata": {},
   "source": [
    "## 12. √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35843872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(examples, features, raw_predictions,\n",
    "                                n_best=20, max_answer_length=30):\n",
    "    \"\"\"\n",
    "    Post-traite les pr√©dictions brutes pour extraire les r√©ponses textuelles.\n",
    "\n",
    "    Args:\n",
    "        examples: Exemples originaux\n",
    "        features: Features tokenis√©es\n",
    "        raw_predictions: Logits de d√©but et fin\n",
    "        n_best: Nombre de candidats √† consid√©rer\n",
    "        max_answer_length: Longueur maximale de la r√©ponse\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping ID -> texte de r√©ponse\n",
    "    \"\"\"\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        context = example[\"context\"]\n",
    "\n",
    "        best_answer = {\"text\": \"\", \"score\": -float(\"inf\")}\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-n_best:][::-1]\n",
    "            end_indexes = np.argsort(end_logits)[-n_best:][::-1]\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (start_index >= len(offset_mapping) or\n",
    "                        end_index >= len(offset_mapping) or\n",
    "                        offset_mapping[start_index] is None or\n",
    "                        offset_mapping[end_index] is None):\n",
    "                        continue\n",
    "\n",
    "                    if (end_index < start_index or\n",
    "                        end_index - start_index + 1 > max_answer_length):\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    text = context[start_char:end_char]\n",
    "                    score = start_logits[start_index] + end_logits[end_index]\n",
    "\n",
    "                    if score > best_answer[\"score\"]:\n",
    "                        best_answer = {\"text\": text, \"score\": float(score)}\n",
    "\n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9dec4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "√âvaluation du mod√®le DeBERTa\n",
      "================================================================================\n",
      "G√©n√©ration des pr√©dictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-traitement des pr√©dictions...\n",
      "\n",
      "================================================================================\n",
      "R√©sultats de l'√©valuation - DeBERTa\n",
      "================================================================================\n",
      "F1 Score: 93.01%\n",
      "Exact Match: 86.58%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âvaluation du mod√®le DeBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"G√©n√©ration des pr√©dictions...\")\n",
    "raw_predictions = trainer.predict(tokenized_validation)\n",
    "\n",
    "print(\"Post-traitement des pr√©dictions...\")\n",
    "final_predictions = postprocess_qa_predictions(\n",
    "    eval_dataset,\n",
    "    tokenized_validation,\n",
    "    raw_predictions.predictions\n",
    ")\n",
    "\n",
    "# Calcul des m√©triques SQuAD\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "formatted_predictions = [\n",
    "    {\"id\": k, \"prediction_text\": v}\n",
    "    for k, v in final_predictions.items()\n",
    "]\n",
    "references = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]}\n",
    "    for ex in eval_dataset\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=formatted_predictions, references=references)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R√©sultats de l'√©valuation - DeBERTa\")\n",
    "print(\"=\"*80)\n",
    "print(f\"F1 Score: {results['f1']:.2f}%\")\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add79a6",
   "metadata": {},
   "source": [
    "## 13. Test d'Inf√©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5eb7edd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '../models/deberta_squad_finetuned' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "The tokenizer you are loading from '../models/deberta_squad_finetuned' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Test d'inf√©rence - DeBERTa\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How large is the Amazon basin?\n",
      "R√©ponse:  7,000,000 square kilometres.\n",
      "Confiance: 0.9725\n",
      "Temps: 10.34 ms\n",
      "\n",
      "Question: What is another name for the Amazon rainforest?\n",
      "R√©ponse:  Amazonia,\n",
      "Confiance: 0.9984\n",
      "Temps: 55.21 ms\n",
      "\n",
      "Temps d'inf√©rence moyen: 32.77 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Test d'inf√©rence - DeBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=OUTPUT_DIR,\n",
    "    tokenizer=OUTPUT_DIR,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "test_context = \"\"\"\n",
    "The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical\n",
    "rainforest in the Amazon biome that covers most of the Amazon basin of South America.\n",
    "The basin is 7,000,000 square kilometres. The rainforest represents over half of the\n",
    "planet's remaining rainforests and comprises the largest and most biodiverse tract\n",
    "of tropical rainforest in the world.\n",
    "\"\"\"\n",
    "\n",
    "test_questions = [\n",
    "    \"How large is the Amazon basin?\",\n",
    "    \"What is another name for the Amazon rainforest?\",\n",
    "]\n",
    "\n",
    "inference_times = []\n",
    "\n",
    "for question in test_questions:\n",
    "    start = time.time()\n",
    "    result = qa_pipeline(question=question, context=test_context)\n",
    "    inference_time = (time.time() - start) * 1000\n",
    "    inference_times.append(inference_time)\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"R√©ponse: {result['answer']}\")\n",
    "    print(f\"Confiance: {result['score']:.4f}\")\n",
    "    print(f\"Temps: {inference_time:.2f} ms\")\n",
    "\n",
    "avg_inference_time = np.mean(inference_times)\n",
    "print(f\"\\nTemps d'inf√©rence moyen: {avg_inference_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee3da0",
   "metadata": {},
   "source": [
    "## 14. Sauvegarde des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29329a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats sauvegard√©s dans: ../models/deberta_squad_finetuned/results.json\n",
      "\n",
      "================================================================================\n",
      "R√©sum√© des r√©sultats - DeBERTa\n",
      "================================================================================\n",
      "model_name: microsoft/deberta-v3-base\n",
      "model_type: deberta\n",
      "finetuned: True\n",
      "f1: 93.01473787274658\n",
      "exact_match: 86.5752128666036\n",
      "training_time_minutes: 133.7895169019699\n",
      "avg_inference_time_ms: 32.77313709259033\n",
      "total_parameters: 183833090\n",
      "trainable_parameters: 183833090\n",
      "num_train_samples: 87599\n",
      "num_eval_samples: 10570\n",
      "num_epochs: 3\n",
      "batch_size: 16\n",
      "learning_rate: 3e-05\n",
      "max_length: 384\n",
      "doc_stride: 128\n"
     ]
    }
   ],
   "source": [
    "results_summary = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"model_type\": \"deberta\",\n",
    "    \"finetuned\": True,\n",
    "    \"f1\": results[\"f1\"],\n",
    "    \"exact_match\": results[\"exact_match\"],\n",
    "    \"training_time_minutes\": training_time / 60,\n",
    "    \"avg_inference_time_ms\": avg_inference_time,\n",
    "    \"total_parameters\": total_params,\n",
    "    \"trainable_parameters\": trainable_params,\n",
    "    \"num_train_samples\": len(train_dataset),\n",
    "    \"num_eval_samples\": len(eval_dataset),\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"doc_stride\": DOC_STRIDE,\n",
    "}\n",
    "\n",
    "output_path = f\"{OUTPUT_DIR}/results.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"R√©sultats sauvegard√©s dans: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R√©sum√© des r√©sultats - DeBERTa\")\n",
    "print(\"=\"*80)\n",
    "for key, value in results_summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11709401",
   "metadata": {},
   "source": [
    "## 15. Comparaison avec Baseline\n",
    "\n",
    "Comparaison des performances avant et apr√®s fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de1e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARAISON: Baseline vs Fine-tuned (DeBERTa)\n",
      "================================================================================\n",
      "M√©trique                         Baseline      Fine-tuned            Gain\n",
      "--------------------------------------------------------------------------------\n",
      "F1 Score (%)                         5.95           93.01          +87.07\n",
      "Exact Match (%)                      0.20           86.58          +86.38\n",
      "================================================================================\n",
      "\n",
      "üéØ Am√©lioration F1: +87.07 points gr√¢ce au fine-tuning!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Charger les r√©sultats baseline\n",
    "baseline_path = \"../models/deberta_baseline/results.json\"\n",
    "if os.path.exists(baseline_path):\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARAISON: Baseline vs Fine-tuned (DeBERTa)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'M√©trique':<25} {'Baseline':>15} {'Fine-tuned':>15} {'Gain':>15}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'F1 Score (%)':<25} {baseline_results['f1']:>15.2f} {results['f1']:>15.2f} {results['f1']-baseline_results['f1']:>+15.2f}\")\n",
    "    print(f\"{'Exact Match (%)':<25} {baseline_results['exact_match']:>15.2f} {results['exact_match']:>15.2f} {results['exact_match']-baseline_results['exact_match']:>+15.2f}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüéØ Am√©lioration F1: +{results['f1']-baseline_results['f1']:.2f} points gr√¢ce au fine-tuning!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Fichier baseline non trouv√©: {baseline_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
